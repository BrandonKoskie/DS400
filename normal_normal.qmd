---
title: "Normal Normal"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(ggplot2)
library(bayesrules)
```

## [5.3 Normal-Normal conjugate family](https://www.bayesrulesbook.com/chapter-5#normal-normal-conjugate-family)

We now have two conjugate families in our toolkit: the Beta-Binomial and the Gamma-Poisson. But many more conjugate families exist! It‚Äôs impossible to cover them all, but there is a third conjugate family that‚Äôs especially helpful to know: the **Normal-Normal**.

Consider a data story. As scientists learn more about brain health, the dangers of concussions (hence of activities in which participants sustain repeated concussions) are gaining greater attention ([Bachynski 2019](https://www.bayesrulesbook.com/chapter-5#ref-bachynski2019no)). Among all people who have a history of concussions, we are interested in Œº, the average volume (in cubic centimeters) of a specific part of the brain: the hippocampus. Though we don‚Äôt have prior information about this group in particular, Wikipedia tells us that among the general population of human adults, both halves of the hippocampus have a volume between 3.0 and 3.5 cubic centimeters.^[38](https://www.bayesrulesbook.com/chapter-5#fn38)^ Thus, the *total* hippocampal volume of *both* sides of the brain is between 6 and 7 cm3. Using this as a starting point, we‚Äôll assume that the mean hippocampal volume among people with a history of concussions, Œº, is also somewhere between 6 and 7 cm3, with an average of 6.5. We‚Äôll balance this prior understanding with data on the hippocampal volumes of n=25 subjects, (Y1,Y2,‚Ä¶,Yn), using the **Normal-Normal Bayesian model**.

No matter the parameters, the Normal model is bell-shaped and symmetric around Œº ‚Äì thus as Œº gets larger, the model shifts to the right along with it. Further, œÉ controls the variability of the Normal model ‚Äì as œÉ gets larger, the model becomes more spread out.

Play with the parameter below

```{r}
plot_normal(mean = 10, sd = 1) +
  geom_vline(xintercept = 9, linetype = "dashed", color = "blue") +
  geom_vline(xintercept = 11, linetype = "dashed", color = "blue") +
  geom_vline(xintercept = 8, color = "coral") +
  geom_vline(xintercept = 12, color = "coral") +
  theme_minimal()
```

Using our understanding of a Normal model, we can now *tune* the prior hyperparameters Œ∏ and œÑ to reflect our prior understanding and uncertainty about the average hippocampal volume among people that have a history of concussions, Œº. Based on our rigorous Wikipedia research that hippocampal volumes tend to be between 6 and 7 cm3, we‚Äôll set the Normal prior mean Œ∏ to the midpoint, 6.5. Further, we‚Äôll set the Normal prior standard deviation to œÑ=0.4. In other words, by [(5.12)](https://www.bayesrulesbook.com/chapter-5#eq:normal-scale), we think there‚Äôs a 95% chance that Œº is somewhere between 5.7 and 7.3 cm3 (6.5¬±2‚àó0.4). This range is *wider*, and hence more conservative, than what Wikipedia indicated. Our uncertainty here reflects the fact that we didn‚Äôt vet the Wikipedia sources, we aren‚Äôt confident that the features for the typical adult translates to people with a history of concussions, and we generally aren‚Äôt sure what‚Äôs going on here (i.e., we‚Äôre not brain experts). Putting this together, our tuned prior model for Œº is:

```{r}
plot_normal(mean = 6.5, sd = 0.4)
```

*Challenge*

-   Add 4 `geom_vline()` to the plot above to indicate 1 standard deviation capturing 68% of the data and 2 standard deviations capturing 95% of the data

```{r}
plot_normal(mean = 10, sd = 1) +
  # ¬±1 standard deviation (68% region)
  geom_vline(xintercept = 9, linetype = "dashed", color = "blue") +
  geom_vline(xintercept = 11, linetype = "dashed", color = "blue") +
  # ¬±2 standard deviations (95% region)
  geom_vline(xintercept = 8, linetype = "dotted", color = "coral") +
  geom_vline(xintercept = 12, linetype = "dotted", color = "coral") +
  theme_minimal()

```

Let‚Äôs apply and examine this result in our analysis of Œº, the average hippocampal volume among people that have a history of concussions. We‚Äôve already built our prior model of Œº, Œº‚àºN(6.5,0.42). Next, consider some data. The `football` data in **bayesrules**, a subset of the `FootballBrain` data in the **Lock5Data** package ([Lock et al. 2016](https://www.bayesrulesbook.com/chapter-5#ref-lock2016statistics)), includes results for a cross-sectional study of hippocampal volumes among 75 subjects ([Singh et al. 2014](https://www.bayesrulesbook.com/chapter-5#ref-singh2014relationship)): 25 collegiate football players with a history of concussions (`fb_concuss`), 25 collegiate football players that do not have a history of concussions (`fb_no_concuss`), and 25 control subjects. For our analysis, we‚Äôll focus on the n=25 subjects with a history of concussions (`fb_concuss`):

```{r}
# Load the data
data(football)
```

*Challenge*

-   Visualize the football data in an interesting way utilizing `ggplot(()`

```{r}
ggplot(football, aes(x = group, y = volume, fill = group)) +
  geom_boxplot(alpha = 0.6, outlier.shape = NA) +
  geom_jitter(width = 0.15, alpha = 0.7, size = 2) +
  labs(
    title = "Hippocampal Volume by Group",
    x = "Group",
    y = "Hippocampal Volume (cm¬≥)"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
ggplot(football, aes(x = years, y = volume, color = group)) +
  geom_point() +
  geom_smooth()
```

Back to our example

```{r}

concussion_subjects <- football %>%
  filter(group == "fb_concuss")
```

```{r}
concussion_subjects %>%
  summarize(mean(volume))
```

```{r}
ggplot(concussion_subjects, aes(x = volume)) + 
  geom_density()
```

```{r}
ggplot(concussion_subjects, aes(x = volume)) + 
  geom_histogram(bins = 5)
```

```{r}
concussion_subjects %>%
  summarize(sd(volume))
```

We plot this likelihood function using `plot_normal_likelihood()`, providing our observed `volume` data and data standard deviation œÉ=0.59 This likelihood illustrates the compatibility of our observed hippocampal data with different Œº values. To this end, the hippocampal patterns observed in our data would most likely have arisen if the mean hippocampal volume across *all* people with a history of concussions, Œº, were between 5.3 and 6.1 cm3. Further, we‚Äôre *most* likely to have observed a mean volume of ¬Ø¬Ø¬Øy= 5.735 among our 25 sample subjects if the underlying population mean Œº were also 5.735.

```{r}
plot_normal_likelihood(y = concussion_subjects$volume, sigma = 0.59) 
```

It is important to note that we are not modeling the distribution of concussion patients hippocampal volume as we did above with `geom_density()`

-   It is important to note that we are plotting **distributions over Œº (the population mean)**, not over individual data

-   likelihood_sd = œÉ/‚àön = 0.59/‚àö25 = 0.12

Bringing all of these pieces together, we plot and summarize our Normal-Normal analysis of Œº using `plot_normal_normal()` and `summarize_normal_normal()` in the **bayesrules** package. Though a compromise between the prior and data, our posterior understanding of Œº is more heavily influenced by the latter. In light of our data, we are much more *certain* about the mean hippocampal volume among people with a history of concussions, and believe that this figure is somewhere in the range from 5.586 to 5.974 cm3 (5.78¬±2‚àó0.097).

```{r}
plot_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.59,
                   y_bar = 5.735, n = 25)
```

```{r}
summarize_normal_normal(mean = 6.5, sd = 0.4, sigma = 0.59,
                   y_bar = 5.735, n = 25)
```

### Challenge

AI Prompting

-   Go back to your modeling scenarios that you created last class

-   Show your data, code, and outputs to an AI assistant

-   Ask it to critique your analysis

    -   Are there alternate/better bayesian methods/models to apply to the question your asking? Think of what we have covered (beta-binomial, gamma-poisson, normal-normal)

    For your fish per cast problem, the Beta‚ÄìBinomial is the most natural Bayesian model:

    It uses the actual number of casts (trials)

    It interprets each cast as a ‚Äúsuccess/failure‚Äù event

    It directly estimates the posterior for ùëù p, the probability of catching a fish on a cast.

    The Gamma‚ÄìPoisson is fine for demonstrating conjugacy, but it glosses over unequal exposure. The Normal‚ÄìNormal is less interpretable here, since fish catches are discrete, bounded, and rates often skewed.

    -   What assumptions are associated with the model you chose? With those assumptions in mind, is the model still a good fit for your question/data?

    Yes the model is a reasonably good fit for your data:

    It naturally incorporates unequal number of casts per trip, which the Gamma‚ÄìPoisson ignored.

    It gives a posterior distribution for the true catch probability, which answers your main question.

    ‚ö†Ô∏è Caveats / limitations:

    Assumes independence between casts; in reality, there could be correlations (e.g., fish school behavior).

    Assumes constant ùëù p within a trip; environmental factors could cause slight variation.

    Bottom line: The Beta‚ÄìBinomial is a much better fit than the Gamma‚ÄìPoisson or Normal‚ÄìNormal for this specific question, but it‚Äôs not perfect. If you had more data or wanted to model variability across trips more explicitly, you could consider a hierarchical Beta‚ÄìBinomial model.

    -   Are there real datasets you can use to answer your question opposed to AI generated datasets?

1.  NOAA Recreational Fisheries Survey Data

What it contains: Catch per unit effort (CPUE), number of fish caught, number of casts, location, species, and angler effort.

Why useful: Provides real counts and effort measures that can be directly modeled with a Beta‚ÄìBinomial or Gamma‚ÄìPoisson framework.

Link: NOAA Fisheries Data

2.  State-Level Fish & Wildlife Angler Surveys

Many states (e.g., Florida, California, Alaska) publish annual or ongoing angler survey datasets.

Example: Florida Fish and Wildlife Conservation Commission (FWC) Recreational Angler Survey.

Variables: Angler trips, casts or effort, number and species of fish caught, sometimes time of day or location.

Use case: Perfect for modeling catch probability per cast or per trip.

3.  Open Fisheries Science Datasets

Kaggle & GitHub: Several projects contain trawl surveys, CPUE data, and catch/effort observations.

Example:

NOAA Northeast Fisheries Survey

GitHub repos like fisheries-data or CPUE-analysis often include CSV files with real counts and effort.
